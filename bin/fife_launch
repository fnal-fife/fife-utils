#!/usr/bin/env python3

try:
    from six import *
except:
    pass

try:
    import configparser
except:
    import ConfigParser as configparser

import traceback
import sys
import os
import optparse

from datetime import datetime

try:
    import urllib.parse, urllib.error

    def urllib_quote(x):
        return urllib.parse.quote(x)


except:
    import urllib

    def urllib_quote(x):
        return urllib.quote(x)

import shutil
try:
    from samweb_client import *
except:
    pass
try:
  from metacat.webapi import MetaCatClient
  from data_dispatcher.api import DataDispatcherClient
except:
  pass

try:
    import poms_client
except:
    pass


def check_exp_err(k, v1, v):
    if v1 and not v:
        raise KeyError("Error: expanding:\n %s: %s \n error in expansion!" % (k, v1))


class launcher(object):
    def __init__(
        self,
        cfgfilename,
        overrides,
        jobname=None,
        stages=[],
        fake=None,
        config_dump=None,
        arg_push=[],
        arg_del=[],
    ):
        self.cfg = configparser.SafeConfigParser()
        # for 3.6..
        # self.cfg.comment_prefixes = ("#",)
        # self.cfg.inline_comment_prefixes = ("#",)
        self.stages = stages
        self.cfg.optionxform = str
        self.is_boolean_flag = {}
        self.included = {}
        self.fake = fake
        self.poms_dd = os.environ.get("POMS_DATA_DISPATCHER_TASK_ID", None) != None
        self.use_dd = self.poms_dd if self.poms_dd else self.cfg.has_section('data_dispatcher') 
        self.use_sam = self.cfg.has_section('sam_consumer') if not self.poms_dd else False
        self.jobname = jobname
        self.searchpath = os.environ.get("FIFE_LAUNCH_PATH", "").split(":")
        self.nfiles_in_dataset = None
        self.poms_flags = {}

        if cfgfilename == None:
            sys.stderr.write("Error: missing required -c configfile option\n")
            sys.exit(1)
        # now pull in the config file with includes
        self.do_cfg_includes(cfgfilename)

        for s in ('global', 'submit', 'executable'):
            if not self.cfg.has_section(s):
                self.cfg.add_section(s)

        if config_dump == "before" or config_dump == "both":
            sys.stderr.write("config before expansion:\n================\n")
            self.cfg.write(sys.stderr)
            sys.stderr.write("\n================\n")

        # handle stage overrides up front
        stages = self.expand_parents(stages)
                
        for stage in stages:
            stagetag = "stage_" + stage
            if self.cfg.has_section(stagetag):
                for k, v in self.cfg_items(stagetag, raw=1):
                    if k == 'parent':
                        continue
                    if not self.cfg.has_section(s):
                        self.cfg.add_section(s)
                    s, n = k.split(".", 1)
                    self.cfg.set(s, n, v)

        # a [global_eval] section so we can do conditionals, etc.
        for k, v in self.cfg_items("global_eval"):
            self.cfg.set("global", k, eval(v))

        # then command line overrides last
        if overrides:
            for o in overrides:
                # print "o is", o
                try:
                    k, v = o.split("=", 1)
                    s, n = k.split(".", 1)
                except:
                    sys.stderr.write("Error: cannot parse '-O%s'\n" % o)
                    sys.stderr.write("   --  expected -Osection.key=value\n")
                    sys.exit(1)

                try:
                    if s[:6] == "stage_":
                        sys.stderr.write(
                            "Warning: Overriding stage value '-O%s' has no effect:\n"
                            % o
                        )
                        sys.stderr.write(
                            "   -- -O changes stage parameter *after* it was used to set %s.\n"
                            % o[o.find(".") + 1 : o.find("=")]
                        )
                        sys.stderr.write("   -- use: -O%s\n" % o[o.find(".") + 1 :])

                    if not self.cfg.has_section(s):
                        self.cfg.add_section(s)

                    self.cfg.set(s, n, v)
                except:
                    sys.stderr.write("Error: cannot store '-O%s'\n" % o)
                    sys.stderr.write("   --  no such section in configuration?\n")
                    sys.exit(1)

        # arg push/parse replace...
        # - put them in an args list
        # - shuffle as requested
        # - stick back in arg_n config entries
        args = []
        for i in range(1, 100):
            try:
                v = self.cfg.get("executable", "arg_%d" % i, raw=True)
            except:
                v = None

            if v != None:
                args.append(v)

        for v in arg_del:
            args.remove(v)

        for v in arg_push:
            args.insert(0, v)

        for i in range(1, 100):
            if i <= len(args):
                self.cfg.set("executable", "arg_%d" % i, args[i - 1])
            else:
                try:
                    self.cfg.remove_option("executable", "arg_%d" % i)
                except:
                    break

        # preset variables referring to environment variables in fife_wrap
        # to save folks typing backslashes
        self.cfg.set("global", "_nthfile", "\\\\\\\${nthfile}")
        self.cfg.set("global", "_fname", "\\\\\\\${fname}")
        self.cfg.set("global", "_furi", "\\\\\\\${furi}")
        self.cfg.set("global", "_uuid", "\\\\\\\${UUID}")
        # values available at job start time
        self.cfg.set("global", "_process", "\${JOBSUBJOBSECTION:-\$PROCESS}")

        if config_dump == "after" or config_dump == "both":
            sys.stderr.write("config after expansion:\n================\n")
            self.cfg.write(sys.stderr)
            sys.stderr.write("\n================\n")

        self.get_fife_wrap_booleans()

        self.cfg_globals = dict(self.cfg.items("global"))

        #
        # fife_launch originally, and for a long time, thought
        # the jobsub option was --dataset and not --dataset_definition
        # (which is an undocumented alias...) so we allow it as a
        # backwards compatability hack...
        #
        dataset = self.cfg_get("submit", "dataset_definition", vars=self.cfg_globals)
        if not dataset:
            dataset = self.cfg_get("submit", "dataset")
            if dataset:
                dataset = self.cfg_get("submit", "dataset", vars=self.cfg_globals)
                self.cfg.set("submit","dataset_definition", dataset)
                self.cfg.remove_option("submit","dataset")
    
        if self.use_dd:
            self.dd_task_id = self.cfg_get("data_dispatcher", "task_id", vars=self.cfg_globals)
            self.dd_proj_id = self.cfg_get("data_dispatcher", "project_id", vars=self.cfg_globals)
            self.dd_dataset_query = self.cfg_get("data_dispatcher", "dataset_query", vars=self.cfg_globals)
            self.dd_param = self.cfg_get("data_dispatcher", "parameter", vars=self.cfg_globals)
            self.dd_namespace = self.cfg_get("data_dispatcher", "namespace", vars=self.cfg_globals)
            self.dd_load_limit = self.cfg_get("data_dispatcher", "load_limit", vars=self.cfg_globals)
            self.dd_server = self.cfg_get("data_dispatcher", "server", vars=self.cfg_globals)
            self.dd_auth_server = self.cfg_get("data_dispatcher", "auth_server", vars=self.cfg_globals)
            self.dd_flags = {}
            self.mc_servers = {
            'dune':('hhttps://metacat.fnal.gov:9443/dune_meta_demo/app',
                    'https://metacat.fnal.gov:8143/auth/dune'),
            'hypot': ('https://metacat.fnal.gov:9443/hypot_meta_dev/app',
                        'https://metacat.fnal.gov:8143/auth/hypot_dev'),
            'mu2e': ('https://metacat.fnal.gov:9443/mu2e_meta_prod/app',
                        'https://metacat.fnal.gov:8143/auth/mu2e')
            }

            self.dd_servers = {
            'dune':('https://metacat.fnal.gov:9443/dune/dd/data',
                    'https://metacat.fnal.gov:8143/auth/dune'),
            'hypot': ('https://metacat.fnal.gov:9443/hypot_dd/data',
                        'https://metacat.fnal.gov:8143/auth/hypot_dev'),
            'mu2e': ('https://metacat.fnal.gov:9443/mu2e_dd_prod/data',
                        'https://metacat.fnal.gov:8143/auth/mu2e')
            }
            if self.poms_dd:
                self.dd_task_id = os.environ.get('POMS_DATA_DISPATCHER_TASK_ID', self.dd_task_id)
                self.dd_proj_id = os.environ.get('POMS_DATA_DISPATCHER_PROJECT_ID', self.dd_proj_id)
                self.dd_dataset_query = os.environ.get('POMS_DATA_DISPATCHER_DATASET_QUERY', self.dd_dataset_query)
                self.dd_param = os.environ.get('POMS_DATA_DISPATCHER_PARAMETER', self.dd_param)
                self.dd_load_limit = os.environ.get('POMS_DATA_DISPATCHER_LOAD_LIMIT', self.dd_load_limit)
        

    def expand_parents(self, stages):
        res = []
        for s in stages:
            p = self.cfg_get("stage_%s"%s, "parent")
            if p:
                res = res + self.expand_parents(p.split(','))
            res.append(s)
        return res

    def get_fife_wrap_booleans(self):
        f = os.popen(
            "%s/libexec/fife_wrap --help"
            % os.environ.get(
                "FIFE_UTILS_DIR", os.path.dirname(os.path.dirname(sys.argv[0]))
            )
        )
        for l in f:
            p1 = l.find("--")
            if p1 >= 0:
                # we have a --option
                p2 = l.find(" ", p1)
                p3 = l.find("=", p1)
                if p3 > 0:
                    # ...--name=value line
                    name = l[p1 + 2 : p3]
                    self.is_boolean_flag[name] = False
                else:
                    # ...--name line (no value)
                    name = l[p1 + 2 : p2]
                    self.is_boolean_flag[name] = True
        f.close()

    def cfg_get(self, section, name, vars={}, fallback=None):
        """ like cfg.get() except it gives None and not an exception """
        try:
            res = self.cfg.get(section, name, vars=vars)
        except:
            res = fallback
        return res

    def cfg_items(self, section, raw=None):
        """ like cfg.items() except it gives [] and not an exception """
        try:
            res = self.cfg.items(section, raw=raw)
        except:
            res = []
        return res

    def do_cfg_includes(self, cffile):
        # print "do_cfg_includes:" , cffile
        found = False
        for d in [".", ""] + self.searchpath:
            if os.path.exists("%s/%s" % (d, cffile)):
                cffile = "%s/%s" % (d, cffile)
                found = True
                break

        if not found:
            sys.stderr.write("Error: -c configfile '%s' not found.\n" % cffile)
            sys.exit(1)

        if self.included.get(cffile, None):
            return
        self.included[cffile] = 1
        self.cfg.read(cffile)
        if self.cfg.has_option("global", "includes"):
            cfls = self.cfg_get("global", "includes")
            for cf in cfls.split(" "):
                if cf:
                    self.do_cfg_includes(cf)

    def get_experiment(self):
        if self.cfg.has_option("env_pass", "SAM_EXPERIMENT"):
            experiment = self.cfg_get(
                "env_pass", "SAM_EXPERIMENT", vars=self.cfg_globals
            )
        elif self.cfg.has_option("global", "experiment"):
            experiment = self.cfg_get(
                "global", "experiment", vars=self.cfg_globals
            )
        elif self.cfg.has_option("global", "group"):
            experiment = self.cfg_get(
                "global", "group", vars=self.cfg_globals
            )
        elif "GROUP" in os.environ or "EXPERIMENT" in os.environ:
            experiment = os.environ.get("GROUP", os.environ["EXPERIMENT"])
        else:
            experiment = self.cfg_get("submit", "G", vars=self.cfg_globals)
        return experiment

    def get_mc_dd_urls(self, experiment):
        experiment = experiment if experiment else self.get_experiment()
        try:
            mc_server = os.environ.get("METACAT_SERVER_URL", self.mc_servers[experiment][0])
            mc_auth_server = os.environ.get("METACAT_AUTH_SERVER_URL", self.mc_servers[experiment][1])
            dd_server = os.environ.get("DATA_DISPATCHER_URL", self.dd_servers[experiment][0])
            dd_auth_server = os.environ.get("DATA_DISPATCHER_AUTH_URL", self.dd_servers[experiment][1])
            return (mc_server, mc_auth_server, dd_server, dd_auth_server)
        except Exception as e:
            message = ['Error: User is attempting to use data dispatcher/metacat, but no server configuration was found.']
            message.append('\tPlease contact fife support, or include the following environment variables into your environment:')
            message.append('\t- METACAT_SERVER_URL')
            message.append('\t- METACAT_AUTH_SERVER_URL')
            message.append('\t- DATA_DISPATCHER_URL')
            message.append('\t- DATA_DISPATCHER_AUTH_URL')
            raise KeyError('\n'.join(message))

    def authorize_mc_dd_client(self):
        try:
            print("Initialize dd/mc client | Begin")
            (mc_server, mc_auth, dd_server, dd_auth) = self.get_mc_dd_urls(self.get_experiment())
            self.mc_client = MetaCatClient(server_url=mc_server, auth_server_url=mc_auth)
            self.dd_client = DataDispatcherClient(server_url=dd_server, auth_server_url=dd_auth)
            print("Initialize dd/mc client | Client's initialized | exp: %s" % self.get_experiment())
            
            # Determine the username to use for login
            if os.environ['USER'] == "%spro" % self.get_experiment() or not self.dd_task_id:
                username = os.environ['USER']
            else:
                username = 'poms'
            print("Initialize dd/mc client | Logging in as: %s " % username)
            
            # Login to the MC/DD clients, verify the credentials are valid
            creds = {
                "metacat": self.mc_client.login_x509(username, os.environ['X509_USER_PROXY']),
                "data_dispatcher": self.dd_client.login_x509(username, os.environ['X509_USER_PROXY'])
            }
            for service, vals in creds.items():
                if not vals or len(vals) != 2:
                    sys.stderr.write("\nInitialize dd/mc client | Login Failed | %s" % service)
                    return False
                if datetime.fromtimestamp(vals[1]) <= datetime.now():
                    sys.stderr.write("\nInitialize dd/mc client | Login Failed | Credentials Expired | %s" % service)
                    return False
        except Exception as e:
            sys.stderr.write("\nInitialize dd/mc client | Failed | Exception: %s" % e)
            return False
        
        print("Initialize dd/mc client | Login Successful")
        return True

    def dd_project_valid(self,limit=None, dataset=None):
        project = None
        print("dd_project_valid | Begin")
        try:
            if self.dd_proj_id:
                print("dd_project_valid | Using existing project_id: %s" % self.dd_proj_id)
                project = self.dd_client.get_project(self.dd_proj_id, True, True) if self.dd_proj_id else None
                self.nfiles_in_dataset = len(project.get("file_handles", []))
            else:
                ## build up or use query
                if self.poms_dd:
                    if not self.dd_dataset_query:
                        sys.stderr.write("Ignoring launch request on empty metacat query")
                        return False
                    # POMS provided a full query already, no need to build one
                    query = self.dd_dataset_query
                    print("dd_project_valid | Using POMS generated query: %s" % query)
                else: 
                    query = 'files from %s where namespace="%s" %s' % (dataset, self.dd_namespace, ' limit %s' % limit if limit else '')
                    print("dd_project_valid | generated query: %s" % query)
                    
                #query metacat - query produces a generator, but we want the list
                query_files = list(self.mc_client.query(query))
                if not query or not query_files: #check query
                    sys.stderr.write("\nIgnoring launch request on empty metacat query")
                    sys.stderr.write("\nQuery: %s"%query)
                    return False

                if len(query_files) > 0:
                    project = self.dd_client.create_project(query_files, query=query)
                    self.dd_proj_id = project['project_id']
                    self.nfiles_in_dataset = len(project.get("file_handles", []))
                else:
                    sys.stderr.write("Ignoring launch request on empty project")
                    return False
                
            if not project or self.nfiles_in_dataset == 0:
                sys.stderr.write("Ignoring launch request on empty project")
                sys.stderr.write("project_id: %s" % self.dd_proj_id)
                return False
            print("dd_project_valid | Found Project | Valid")
            return True
        
        except Exception as e:
            sys.stderr.write("Get Data Dispatcher Project | Exception: %s" % e)
            
        return False
        
        

    def pre_launch(self, dry_run=False):
        print("Doing Prelaunch")
        for i in range(30):
            if i > 0:
                suffix = "_%d" % i
            else:
                suffix = ""

            dest = self.cfg_get("job_output" + suffix, "dest", vars=self.cfg_globals)
            uniqflag = self.cfg_get(
                "job_output" + suffix, "dest_uniq_rename", vars=self.cfg_globals
            )

            no_checkdir = self.cfg_get(
                "job_output" + suffix, "launch_dest_check", vars=self.cfg_globals
            ) in ("False", "false","f","0")

            if no_checkdir:
                continue

            ## do mkdir with the right experiment set...
            exp = self.get_experiment()
            if exp and not os.environ.get("EXPERIMENT", None):
                os.environ["EXPERIMENT"] = exp

            if dest and not uniqflag and not dest.find("$") >= 0:
                if dry_run:
                    print("I would: ifdh mkdir_p %s" % dest)
                else:
                    import ifdh
                    ih = ifdh.ifdh()
                    ih.mkdir_p(dest)

        
        if self.poms_dd:
            print("Using Data Dispatcher | POMS Variant")
            self.use_sam = False
            self.dd_task_id = os.environ.get('POMS_DATA_DISPATCHER_TASK_ID', self.dd_task_id)
            self.dd_proj_id = os.environ.get('POMS_DATA_DISPATCHER_PROJECT_ID', self.dd_proj_id)
            self.dd_dataset_query = os.environ.get('POMS_DATA_DISPATCHER_DATASET_QUERY', self.dd_dataset_query)
            self.dd_param = os.environ.get('POMS_DATA_DISPATCHER_PARAMETER', self.dd_param)
            self.dd_load_limit = os.environ.get('POMS_DATA_DISPATCHER_LOAD_LIMIT', self.dd_load_limit)
            if not (self.dd_proj_id or self.dd_dataset_query):
                sys.stderr.write("ERROR: USING DATA DISPATCHER -- NEED TO PROVIDE project_id, or dataset_query, or BOTH dataset AND namespace WITHIN SUBMIT SECTION")
                sys.exit(1)
            if not self.authorize_mc_dd_client() or not self.dd_project_valid():
                sys.exit(1)

            self.poms_flags["POMS_DATA_DISPATCHER_TASK_ID"] = self.dd_task_id
            self.poms_flags["POMS_DATA_DISPATCHER_PROJECT_ID"] = self.dd_proj_id
            if self.dd_dataset_query:
                self.poms_flags["POMS_DATA_DISPATCHER_DATASET_QUERY"] = self.dd_dataset_query
            if self.dd_param:
                self.poms_flags["POMS_DATA_DISPATCHER_PARAMETER"] = self.dd_param
            if self.dd_load_limit:
                self.poms_flags["POMS_DATA_DISPATCHER_LOAD_LIMIT"] = self.dd_load_limit
            
        elif self.use_dd:
            print("Using Data Dispatcher")
            dataset = self.cfg_get("data_dispatcher", "dataset", vars=self.cfg_globals)
            self.dd_task_id = self.cfg_get("data_dispatcher", "task_id", vars=self.cfg_globals)
            self.dd_proj_id = self.cfg_get("data_dispatcher", "project_id", vars=self.cfg_globals)
            self.dd_dataset_query = self.cfg_get("data_dispatcher", "dataset_query", vars=self.cfg_globals)
            self.dd_param = self.cfg_get("data_dispatcher", "parameter", vars=self.cfg_globals)
            self.dd_namespace = self.cfg_get("data_dispatcher", "namespace", vars=self.cfg_globals)
            self.dd_load_limit = self.cfg_get("data_dispatcher", "load_limit", vars=self.cfg_globals)
            limit = self.cfg_get("data_dispatcher", "query_limit", vars=self.cfg_globals)
            if not (self.dd_proj_id or (dataset and self.dd_namespace) or self.dd_dataset_query):
                sys.stderr.write("ERROR: USING DATA DISPATCHER -- NEED TO PROVIDE project_id, dataset_query, or BOTH dataset AND namespace WITHIN SUBMIT SECTION")
                sys.exit(1)
            if not self.authorize_mc_dd_client() or not self.dd_project_valid(limit=limit, dataset=dataset):
                sys.exit(1)
                
        elif self.use_sam:
            print("Using SAM Consumer")
            dataset = self.cfg_get("submit", "dataset_definition", vars=self.cfg_globals)
            if dataset:
                if dry_run:
                    print("I would: samweb count_definition_files %s" % dataset)
                    print("and make sure its non-zero...")
                    self.nfiles_in_dataset = 5
                else:
                    samweb = SAMWebClient(experiment=self.get_experiment())
                    self.nfiles_in_dataset = samweb.countFiles(defname=dataset)
                    if self.nfiles_in_dataset == 0:
                        print("Ignoring launch request on empty dataset")
                        sys.exit(1)
        else:
            print("Ignoring launch request, No data_dispatcher or sam_consumer section found in configuration")
            sys.exit(1)

        for i in range(100):
            cmd = self.cfg_get("prelaunch", "script" if i == 0 else "script_%d" % i, vars=self.cfg_globals)
            if not cmd:
                break
            elif dry_run:
                print("I would run: %s" % cmd)
            else:
                os.system(cmd)

    def launch(self, dry_run=False):
        if dry_run:
            print("Pre-launch checks:")
            self.pre_launch(dry_run=True)
            # trim down to command and args from jobsub line
            cmd = self.build_launch()
            print("Then I would run: \n-----\n%s\n-----" % cmd)
            print("... which would, in the job, do:\n-----")
            cmd = cmd[cmd.find("file://") + 8 :]
            cmd = cmd.replace(" ", " --dry_run ", 1)
            os.system(cmd)
            print("-----")
        else:
            self.pre_launch()
            cmd = self.build_launch()
            print(("running:\n %s", cmd))
            os.system(cmd)

        if self.jobname != None:
            os.unlink(self.jobwrapfile)

    def build_launch(self):
        res = []
        
        res.append("set -x")

        #
        # if we are doing our own POMS stuff, hide poms_jobsub_wrapper's
        # version of jobsub_submit, by pushing jobsub_client's bin
        # back on the front of the path.
        #
        if self.cfg.has_section("poms_get_task_id"):
            res.append("PATH=$JOBSUB_CLIENT_DIR:$PATH")

        res.append(
            "GROUP=%s; export GROUP"
            % self.cfg_get("global", "group", vars=self.cfg_globals)
        )

        # for k,v in self.cfg_items("env_pass", raw = 1):
        #    v = self.cfg_get("env_pass",k,vars = self.cfg_globals)
        #    res.append("%s='%s'; export %s" % (k, v, k))
        if self.cfg.has_section("submit"):

            nfpj = self.cfg_get("submit", "n_files_per_job", vars=self.cfg_globals)
            if nfpj and int(nfpj) > 0:
                # we already counted the dataset files in pre_launch,
                # so just compute
                if self.nfiles_in_dataset != None:
                    self.cfg.set("submit", "N", str(int((self.nfiles_in_dataset + int(nfpj)-1) / int(nfpj))))

                else:
                    sys.stderr.write(
                        "Error in config [submit]: n_jobs_per_file requires dataset\n"
                    )
                    sys.exit(1)


            # env_pass part one -- export the VAR='val' before
            # starting jobsub
            for k, v1 in self.cfg_items("env_pass", raw=1):
                v = self.cfg_get("env_pass", k, vars=self.cfg_globals)
                check_exp_err(k, v1, v)
                if "POMS" in k and v:
                    if k not in self.poms_flags:
                        self.poms_flags[k] = v
                else:
                    res.append("export %s='%s'" % (k, v))
                    
                    
            uses_poms = len(self.poms_flags) > 0
            for key, val in os.environ.items():
                if key in ['POMS_ENV', "POMS_CAMPAIGN_ID", "POMS_TASK_ID"] or key.startswith("POMS_DATA_DISPATCHER_") and val:
                    uses_poms = True
                    print("adding %s=%s" %(key,val))
                    if key not in self.poms_flags:
                        self.poms_flags[key] = val
                        
            if uses_poms:            
                for k, v in self.poms_flags.items():
                    if "export %s='%s'" % (k, v) not in res:
                        res.append("export %s=%s " % (k, v) 
                                   if k not in ["POMS_DATA_DISPATCHER_DATASET_QUERY", "POMS_DATA_DISPATCHER_PARAMETER"] 
                                   else
                                   "export %s='%s'" % (k, v) 
                                   )
                    
            if self.fake:
                res.append("fake_jobsub \\")
            else:
                res.append("jobsub_submit \\")

            # env_pass part two: pass -e VAR for each var
            for k, v1 in self.cfg_items("env_pass", raw=1):
                res.append("  -e %s \\" % k)
            
            if uses_poms:            
                for k, v in self.poms_flags.items():
                    if "  -e %s \\" % k  not in res:
                        res.append("  -e %s \\" % k)

            if self.cfg.has_section("poms_campaign_layer"):
                #
                # do the full poms_client game:
                # -- register the campaign
                # -- request a task_id
                #
                campaign_kwargs = {}
                for k, v1 in self.cfg_items("poms_campaign_layer", raw=1):
                    v = self.cfg_get("poms_campaign_layer", k, vars=self.cfg_globals)
                    check_exp_err(k, v1, v)
                    campaign_kwargs[k] = v

                campaign_id = poms_client.register_poms_campaign(**campaign_kwargs)
                poms_get_id_kwargs = {"campaign": campaign_id}
            else:
                poms_get_id_kwargs = {}

            for k, v1 in self.cfg_items("submit", raw=1):
                if k == "n_files_per_job":
                    continue
                v = self.cfg_get("submit", k, vars=self.cfg_globals)
                check_exp_err(k, v1, v)
                if len(k) > 2 and k[-2] == "_" and k[-1] in "0123456789":
                    k = k[:-2]
                elif (
                    len(k) > 3
                    and k[-3] == "_"
                    and k[-1] in "0123456789"
                    and k[-2] in "0123456789"
                ):
                    k = k[:-3]
                if len(k) == 1:
                    res.append("  -%s %s  \\" % (k, v))
                elif v == "True" or v == True:
                    res.append("  --%s \\" % k)
                else:
                    res.append("  --%s=%s  \\" % (k, v))

            defaultwrap = "%s/libexec/fife_wrap" % os.environ['FIFE_UTILS_DIR'] 

            wrapfile = os.path.expandvars(
                self.cfg_get("global", "wrapper", vars=self.cfg_globals, fallback=defaultwrap)
            )
            if wrapfile.startswith("file:///"):
                wrapfile = wrapfile[7:]

            if not self.jobname and self.cfg_get("global", "jobname"):
                self.jobname = self.cfg_get("global", "jobname")

            if self.jobname != None:
                self.jobwrapfile = "/tmp/%s_%d" % (self.jobname, os.getpid())
                print("DEBUG: copying %s to %s " % (wrapfile, self.jobwrapfile))
                shutil.copy(wrapfile, self.jobwrapfile)
            else:
                self.jobwrapfile = wrapfile

            res.append("  file:///%s \\" % self.jobwrapfile)

            for k, v1 in self.cfg_items("job_setup", raw=1):
                v = self.cfg_get("job_setup", k, vars=self.cfg_globals)
                check_exp_err(k, v1, v)
                if len(k) > 2 and k[-2] == "_" and k[-1] in "0123456789":
                    k = k[:-2]
                elif (
                    len(k) > 3
                    and k[-3] == "_"
                    and k[-1] in "0123456789"
                    and k[-2] in "0123456789"
                ):
                    k = k[:-3]
                if self.is_boolean_flag.get(k, False):
                    if v == "True" or v == True:
                        res.append("    --%s \\" % k)
                else:
                    # use -unquote version of all these, always, to
                    # preserve the order, otherwise the unquoted ones
                    # get run before the quoted ones which looks random
                    # to users.
                    if (
                        k in ["export", "source", "setup", "prescript", "postscript", "finally", "spack-load", "spack-env-activate"]
                        and v
                    ):
                        k = "%s-unquote" % k
                        v = urllib_quote(v)
                    res.append("    --%s '%s' \\" % (k, v))

            for k, v1 in self.cfg_items("sam_consumer", raw=1):
                v = self.cfg_get("sam_consumer", k, vars=self.cfg_globals)
                check_exp_err(k, v1, v)
                if self.is_boolean_flag[k]:
                    if v == "True" or v == True:
                        res.append("  --%s \\" % k)
                else:
                    res.append("    --%s '%s' \\" % (k, v))

            if self.use_dd:
              res.append('    --data_dispatcher \\')
              res.append('    --data_dispatcher_project %i \\'% int(self.dd_proj_id))
              res.append('    --data_dispatcher_namespace %s \\'%self.dd_namespace)
              res.append('    --data_dispatcher_load_limit %s \\'%self.dd_load_limit)
              res.append('    --data_dispatcher_user %s \\'%os.environ['USER'])

              wait_limit = self.cfg_get("data_dispatcher", "wait_limit", vars=self.cfg_globals)
              if wait_limit:
                res.append('    --data_dispatcher_wait_limit %s \\'%wait_limit) 

              wait_time = self.cfg_get("data_dispatcher", "wait_time", vars=self.cfg_globals)
              if wait_time:
                res.append('    --data_dispatcher_wait_time %s \\'%wait_time) 

              timeout = self.cfg_get("data_dispatcher", "timeout", vars=self.cfg_globals)
              if timeout:
                res.append('    --data_dispatcher_timeout %s \\'%timeout)

            for i in range(30):
                if i == 0:
                    ssuffix = ""
                    osuffix = ""
                else:
                    ssuffix = "_%d" % i
                    osuffix = "%d" % i

                if not self.cfg.has_section("job_output" + ssuffix):
                    break

                for k, v1 in self.cfg_items("job_output" + ssuffix, raw=1):
                    v = self.cfg_get("job_output" + ssuffix, k, vars=self.cfg_globals)
                    check_exp_err(k, v1, v)

                    if len(k) > 2 and k[-2] == "_" and k[-1] in "0123456789":
                        k = k[:-2]
                    elif (
                        len(k) > 3
                        and k[-3] == "_"
                        and k[-1] in "0123456789"
                        and k[-2] in "0123456789"
                    ):
                        k = k[:-3]

                    # some flags don't get passed on...
                    if k in ('launch_dest_check',):
                        continue

                    if self.is_boolean_flag.get(k, False):
                        if v == "True" or v == True:
                            res.append("    --%s%s \\" % (k, osuffix))
                    else:
                        if k in ["metadata_extractor"] and v and urllib_quote(v) != v:
                            k = "%s_unquote" % k
                            v = urllib_quote(v)
                        res.append("    --%s%s '%s' \\" % (k, osuffix, v))

            for i in range(100):
                if i == 0:
                    sname = "executable"
                else:
                    sname = "executable_%d" % i

                if not self.cfg.has_section(sname):
                    continue

                res.append(
                    "  --exe  %s \\"
                    % self.cfg_get(sname, "name", vars=self.cfg_globals)
                )
                if self.cfg_get(sname, "exe_stdout", vars=self.cfg_globals):
                    res.append(
                        "  --exe_stdout%d %s \\"
                        % (i, self.cfg_get(sname, "exe_stdout", vars=self.cfg_globals))
                    )

                if self.cfg_get(sname, "exe_stderr", vars=self.cfg_globals):
                    res.append(
                        "  --exe_stderr%d %s \\"
                        % (i, self.cfg_get(sname, "exe_stderr", vars=self.cfg_globals))
                    )

            for i in range(100):
                if i == 0:
                    sname = "executable"
                else:
                    sname = "executable_%d" % i

                if not self.cfg.has_section(sname):
                    continue

                res.append(" -- \\")  # end options to wrapper

                for k, v1 in self.cfg_items(sname, raw=1):
                    v = self.cfg_get(sname, k, vars=self.cfg_globals)
                    check_exp_err(k, v1, v)
                    if k.startswith("arg_"):
                        res.append("      %s \\" % v)

            if self.cfg.has_section("poms_get_task_id"):

                if os.environ.get("POMS_TEST", "") != "":
                    poms_get_id_kwargs["test"] = os.environ["POMS_TEST"]

                if os.environ.get("POMS_PARENT_TASK_ID", "") != "":
                    poms_get_id_kwargs["parent_task_id"] = os.environ[
                        "POMS_PARENT_TASK_ID"
                    ]
                poms_get_id_kwargs["command_executed"] = "\n".join(res)

                for k, v1 in self.cfg_items("poms_get_task_id", raw=1):
                    v = self.cfg_get("poms_get_task_id", k, vars=self.cfg_globals)
                    check_exp_err(k, v1, v)
                    poms_get_id_kwargs[k] = v

                #
                # looks redundant, allows override from config file
                # or value from register_poms_campaign
                #
                campaign_id = poms_get_id_kwargs["campaign"]

                task_id = poms_client.get_task_id_for(**poms_get_id_kwargs)

                n = res.index("jobsub_submit \\") + 1

                res.insert(n, " -e POMS_CAMPAIGN_ID=%s \\" % campaign_id)
                res.insert(n, " -e POMS_TASK_ID=%s \\" % task_id)
                res.insert(
                    n,
                    """ -l "+FIFE_CATEGORIES='\\"POMS_TASK_ID_%s,POMS_CAMPAIGN_ID_%s,%s\\"'"  \\"""
                    % (task_id, campaign_id, os.environ.get("POMS_CAMPAIGN_TAGS", "")),
                )
                res.insert(n, ' -l "+POMS_TASK_ID=%s"  \\' % task_id)
                res.insert(n, ' -l "+POMS_CAMPAIGN_ID=%s"  \\' % campaign_id)

            res.append("")

        elif self.cfg.has_section("submit_dag"):
            # XXX not doing dags -- yet
            print("submit_dag not yet implemented...")
            pass
        else:
            raise KeyError("Need [submit] or [submit_dag] sections...")

        return "\n".join(res)


if __name__ == "__main__":
    p = optparse.OptionParser(
        description="launch a job with parameters",
        prog="fife_launch",
        version="1.1",
        usage="%prog --config file.cfg --override foo.bar=10",
    )

    p.add_option("-c", "--config", help="config file describing job")
    p.add_option(
        "-O", "--override", help="override config file values", action="append"
    )
    p.add_option("--jobname", help="job name for launching", default=None)
    p.add_option(
        "--stage", action="append", help="stage name to add options from", default=[]
    )
    p.add_option("--fake", action="store_true", help="use fake_jobsub", default=None)
    p.add_option(
        "--dry_run",
        action="store_true",
        help="Just print what we would do",
        default=None,
    )
    p.add_option(
        "--config_dump",
        help='dump our config "before" or "after" expansion and overrides (or "both")',
        default=None,
    )
    p.add_option(
        "--arg_push",
        action="append",
        help="push a command line argument in the executable section",
        default=[],
    )
    p.add_option(
        "--arg_del",
        action="append",
        help="delete a specific argument in the executable section",
        default=[],
    )
    options, args = p.parse_args()

    if len(args) > 0:
        sys.stderr.write("Error: extra command line arguments %s\n" % repr(args))
        sys.stderr.write("-- perhaps you are missing some quotes?")

        sys.exit(1)

    try:
        l = launcher(
            options.config,
            options.override,
            options.jobname,
            options.stage,
            options.fake,
            options.config_dump,
            options.arg_push,
            options.arg_del,
        )
        # evaluate some things to make sure we have them...
        l.cfg.items("global", raw=1)
        l.cfg.items("submit", raw=1)
        l.cfg.items("executable", raw=1)
    except configparser.Error as e:
        sys.stderr.write("Error in config file(s): %s\n" % e)
        raise
        sys.exit(1)
    except Exception as e:
        sys.stderr.write("Error in setup: %s\n" % e)
        traceback.print_exc()
        sys.exit(1)

    l.launch(options.dry_run)
