#!/usr/bin/env python 

import os
import socket
import grp

from fife_sam_utils import log_startup, log_finish

try:
   import uuid
except:
   class uuid:
       def uuid4():
         return "%s-%s-%s-%s-%s" % (
              hex(int(time.time()))[2:],
              hex(os.getpid())[2:],
              hex(os.getppid())[2:],
              hex(os.getpid())[2:],
              ''.join([hex(int(x))[2:] for x in socket.gethostbyname(socket.gethostname()).split('.')])
            )
       uuid4 = staticmethod(uuid4)

class condenser:
    def __init__(self, options):
        self.options = options

    def startproject_command(self):
        return "ifdh startProject %s ''  %s %s %s" % (
                 self.options.projname, self.options.name, self.options.user, self.options.group )

    def endproject_command(self):
        return "ifdh endProject `ifdh findProject %s `" % self.projname

    def start_consumer_command(self):
        return """
             cpurl=`ifdh findProject %s`
             cpid=`ifdh establishConsumer "$cpurl" "demo" "$ART_VERSION" "$hostname" "${GRID_USER:-$USER}" "art" "" "$JOBSUBJOBID" xrootd`
        """ % self.projname

    def condense_command(self):
        if self.options.artflag:
            return self.basecommand + " --sam-web-uri=$cpurl --sam-process-id=$cpid ---process-name=test"

    def condense2_command(self):
        return self.options.phase_2

    def cleanup(self):
        os.system("rm -rf %s" % self.workdir())

    def wait_for_job(self):
        """ watch with jobsub_q for job to finish..."""
        still_running = True
        while still_running:
	    still_runing = False
            time.sleep(10)
            f = os.popen("jobsub_q --group %s --jobid=%s" % (self.group, self.jobid))
            for line in f:
	        if line.startswith(self.jobid):
                    still_runing = True
            f.close()
        
    def launch_condense_dag(self, njobs):

        """ build 3-stage DAG and run it """

	os.mkdir(self.workdir)

	f = open("%s/startproj.sh" % self.workdir,"w")
	f.write(self.setupstring)
	f.write(self.startproject_command())
	f.close()

	f = open("%s/condense1.sh" % self.workdir,"w")
	f.write(self.setup_string)
	f.write(self.start_consumer_command())
	f.write(self.condense_command())
	f.write("ifdh addOutput %s" %  self.outputglob)
	f.write("ifdh renameOutput unique")
	f.write("ifdh copyBackOutput %s" % self.scratchdir)
	f.close()

	f = open("%s/condense2.sh" % self.workdir,"w")
	f.write(self.setup_string)
	f.write("ifdh ls %s 1 > flist " % self.scratchdir)
        if options.xrootd:
	    f.write("sed -e 's;.*/pnfs/;xrootd://fndca1.fnal.gov:1904/pnfs;' < flist > ulist " % self.scratchdir)
        else:
            f.write("sed -e 's;\(.*\)/\(.*\);& \2;' < flist > cplist")
            f.write("sed -e 's;\(.*\)/\(.*\);\2;' < flist > ulist")
            f.write("ifdh cp -f cplist")
	f.write(self.condense2_command() + ' `cat ulist`' )
	f.write("ifdh addOutput %s" %  self.outputglob)
	f.write("ifdh copyBackOutput %s" % self.scratchdir)
	f.write(self.endproject_command())
	f.close()

	f = open("%s/condense.dag" % self.workdir, "w")
	f.write("<serial>\n")
	f.write("jobsub file://%s/startproj.sh\n" % self.workdir)
	f.write("jobsub -N %d file://%s/condense1.sh\n" % (njobs, self.workdir))
	f.write("jobsub file://%s/condense2.sh\n" % self.workdir)
	f.write("</serial>\n")
	f.close()

	f = os.popen("jobsub_submit_dag --group=%s --resource-provides=usage_model=OPPORTUNISTIC file://%s/condense.dag" % (self.group, self.workdir), "r")
	for line in f:
	    if line.startswith("JobsubJobID of first job:"):
		self.jobid = line[26:]
	f.close()
	self.wait_for_job()
        self.cleanup()

    def condense_locally():
       os.system(self.startproject_command())
       time.sleep(1)
       os.system(self.start_consumer_command())
       os.system(self.condense_command())
       os.system(self.endproject_command())

    def condense_it():
        n = samweb.count_datset(self.dataset)
        if (n < 50 or self.force_local):
            condense_locally()
        else:
            launch_condense_dag(int(sqrt(n)))
        

if __name__ == '__main__':
    import optparse
    import time

    log_startup()

    experiment = os.environ.get('EXPERIMENT',os.environ.get('SAM_EXPERIMENT',grp.getgrgid(os.getgid())[0]))

    parser = optparse.OptionParser(usage="usage: %prog [options] \n slurp dataset through program and generate summed files")
    parser.add_option('-p','--projname', default='condense-%s'%uuid.uuid4())
    parser.add_option('-v','--verbose',   action='count')
    parser.add_option('-e','--experiment', default=experiment, help='use this experiment server defaults to $SAM_EXPERIMENT if not set')
    parser.add_option('-b','--batch_size',default = 50, type='int', help="run in batches of this size")
    parser.add_option('-n', '--name',     help="dataset name to sum" )
    parser.add_option('-1', '--phase-1',  help="command to generate summary" )
    parser.add_option('-2', '--phase-2',  help="command to sum summary files" )
    parser.add_option('-w', '--work',     help="working directory location" )
    parser.add_option('-a', '--art',      help="phase 1 command is an art executable", action='store_true')

    (o,a) = parser.parse_args()

    if o.experiment:
        os.environ['EXPERIMENT']= o.experiment
        os.environ['SAM_EXPERIMENT']= o.experiment
        os.environ['IFDH_BASE_URI'] = "http://samweb.fnal.gov:8480/sam/%s/api" % o.experiment

    else: 
        sys.stderr.write("Error: Need either --experiment or $EXPERIMENT $SAM_EXPERIMENT in environment")
        os.exit(1)

    cert = get_standard_certificate_path(o)
    os.environ['X509_USER_PROXY'] = cert

    if not o.name:
       parser.error("expected --name dataset-name")

    if not o.phase_1:
       parser.error("expected --phase-1 program-name")

    if not o.phase_2:
       parser.error("expected --phase-2 program-name")

    c = condenser(o)
    c.condense_it()

    log_finish("Success")
