#!/usr/bin/env python
try:
    from six import *
except:
    pass

import sys
import os
import grp
import optparse
import logging
import ifdh
from samweb_client import *
from fife_sam_utils import *


class de_duper:
    def __init__(self):
        self.metadata_list = [
            "file_format",
            "file_type",
            "data_tier",
            "user",
            "application",
            "parents",
        ]

    def is_dup(self, md1, md2):

        for k in self.metadata_list:
            if md1.get(k, None) != md2.get(k, None):
                return 0
        return 1

    def include_metadata(self, metadata_list):
        self.metadata_list.extend(metadata_list)

    def extract_key(self, metadata):

        # parent file id is primary sort key
        res = [str(metadata.get("parents", [{"file_id": -1}])[0]["file_id"])]
        # tack on metadata fields we compare on, so they'll be sorted that way
        for k in self.metadata_list:
            res.append(repr(metadata.get(k, "")))

        return ":".join(res)

    def duplicate_kids(self, ds, verbose=False, experiment=None, keeplists=None):
        self.samweb = SAMWebClient(experiment=experiment)
        dupl = {}
        fl = []

        for f in ds.file_iterator():
            fl.append(f)

        mdl = []
        while fl:
            md_json = self.samweb.getMultipleMetadata(fl[:500], asJSON=True)
            mdl.extend(json.loads(md_json))
            fl = fl[500:]

        # order metadata list by (first) parent file id

        mdl.sort(key=self.extract_key)

        i = 0
        j = 1
        oldp = None
        dupl[None] = {}
        n = len(mdl)
        while i < n and j < n:
            dl = []
            p = mdl[i]["parents"][0]["file_name"]
            if p != oldp:
                dupl[p] = {}
            oldp = p

            while j < n and self.is_dup(mdl[i], mdl[j]):
                # list the smaller file as the duplicate
                # if a file is zero size, or truncated, the longer one
                # is the keeper...
                if mdl[i]["file_size"] < mdl[j]["file_size"]:
                    dl.append(mdl[i]["file_name"])
                    i = j
                else:
                    dl.append(mdl[j]["file_name"])
                j = j + 1

            # print("parent %s file %s dups %s" % (p, mdl[i]['file_name'], repr(dl)))
            dupl[p][mdl[i]["file_name"]] = dl

            i = j
            j = i + 1

        return dupl


if __name__ == "__main__":

    log_startup()

    experiment = os.environ.get(
        "EXPERIMENT", os.environ.get("SAM_EXPERIMENT", grp.getgrgid(os.getgid())[0])
    )

    parser = optparse.OptionParser(
        usage="usage: %prog [options] --dims dimensions \n Check files in dims for duplicate children of same parent"
    )
    parser.add_option("-v", "--verbose", action="count", default=0)
    parser.add_option(
        "-e",
        "--experiment",
        default=experiment,
        help="use this experiment server defaults to $SAM_EXPERIMENT if not set",
    )
    parser.add_option("--dims", help="dimension query for files to check")
    parser.add_option(
        "--include_metadata",
        help="metadata field to include in comparisons",
        default=[],
        action="append",
    )
    parser.add_option(
        "--mark_bad",
        action="store_true",
        default=False,
        help="mark as 'bad' in content_status",
    )
    parser.add_option(
        "--retire_file",
        action="store_true",
        default=False,
        help="retire duplicate files",
    )
    parser.add_option(
        "--delete", action="store_true", default=False, help="delte duplicate files"
    )

    (o, a) = parser.parse_args()

    if o.verbose > 1:
        logging.basicConfig(level=logging.DEBUG)
    elif o.verbose > 0:
        logging.basicConfig(level=logging.INFO)
    else:
        logging.disable(logging.INFO)

    if o.experiment:
        os.environ["EXPERIMENT"] = o.experiment
        os.environ["SAM_EXPERIMENT"] = o.experiment
        os.environ["IFDH_BASE_URI"] = (
            "http://sam%s.fnal.gov:8480/sam/%s/api" % 
               (o.experiment, o.experiment)
               .replace("samsamdev","samdev")
        )
    else:
        sys.stderr.write(
            "Error: Need either --experiment or $EXPERIMENT $SAM_EXPERIMENT in environment"
        )
        sys.exit(1)

    cert = get_standard_certificate_path(o)
    os.environ["X509_USER_PROXY"] = cert

    if not o.dims:
        parser.error("expected --dims dimensions")
        exit(1)

    dd = de_duper()

    dd.include_metadata(o.include_metadata)

    res = dd.duplicate_kids(
        dataset(dims=o.dims), verbose=o.verbose, experiment=o.experiment
    )

    ih = ifdh.ifdh()

    for p in res:
        has_dups = False
        for k in res[p]:
            if res[p][k]:
                has_dups = True
        if not has_dups:
            continue
        print("parent %s:" % p)
        for k in res[p]:
            if not res[p][k]:
                continue
            print("  duplicates of %s:" % k)
            for f in res[p][k]:
                acts = ""
                if o.delete:
                    ll = dd.samweb.locateFile(f)
                    for l in ll:
                        try:
                            path = sampath(l["full_path"] + "/" + f)
                            ih.rm(path, "")
                            acts += "(deleted)"
                        except Exception as e:
                            acts += "(failed deleting %s)" % path

                if o.mark_bad:
                    dd.samweb.modifyFileMetadata(f, md={"content_status": "bad"})

                    acts += "(marked bad)"
                if o.retire_file:
                    dd.samweb.retireFile(f)
                    acts += "(retired)"

                print("    %s %s" % (f, acts))

    log_finish("Success")
    sys.exit(0)
